<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- <meta http-equiv="X-UA-Compatible" content="IE=edge"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>WsUID-Net Project Page</title>
    <!-- Bootstrap -->
    <link href="./bootstrap-4.0.0.css" rel="stylesheet">
</head>

<body data-new-gr-c-s-check-loaded="14.1036.0" data-gr-ext-installed="">
    <div id="page_container">
        <header>
            <div class="jumbotron">
                <div class="container">
                    <div class="row">
                        <div class="col-12">

                            <h1 class="text-center"> Learning Scribbles for Dense Depth: Weakly-Supervised Single Underwater Image Depth Estimation Boosted by Multi-Task Learning</h1>
                            <p class="text-center">&nbsp;</p>
                            <h5 class="text-center"><a href="https://Wangxy97.github.io/">Kunqian Li<sup>1</sup></a>, <a href="https://Wangxy97.github.io/">Xiya Wang<sup>2</sup></a>, <a href="http://ouc.ai/">Wenjie Liu<sup>1</sup></a>, <a href="https://trentqq.github.io/">QI QI<sup>2</sup></a>,
                                Zhiguo Zhang<sup>3</sup>, Kun Sun<sup>4</sup></h5>
                            <p class="text-center"><sup>1</sup>College of Information Science and Engineering, Ocean University of China</p>
                            <p class="text-center"><sup>2</sup>College of Engineering, Ocean University of China</p>
                            <p class="text-center"><sup>3</sup>College of Computer Science and Technology, Qingdao University</p>
                            <p class="text-center"><sup>4</sup>School of Computer Science, China University of Geosciences</p>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <section>


            <div class="container">
                <p>&nbsp;</p>
                <div class="row">
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
                        <h2>Abstract</h2>
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12 col-md-12 col-sm-12 text-center  offset-xl-0 col-xl-12">
                        <p style="text-align:justify"><em>Estimating depth from a single underwater image is one of the main tasks of underwater visual perception. However, data-driven underwater depth estimation methods have long been difficult to make breakthroughs due to the difficulty of obtaining a large number of true-value references. This is partly due to the high cost of acquisition equipment, which is difficult to be applied to diverse ocean scenes by a wide range of users, and therefore sample diversity is difficult to guarantee; on the other hand, manual annotation of dense depth relationships is almost impossible to achieve. In this paper, we establish a new underwater depth estimation benchmark, namely SUIM-SDA, by extending the SUIM dataset with more than 6000 manually annotated depth trendlines and 25 million depth-ranked pixel pairs. Using the sparse depth relation annotation provided by SUIM-SDA and the semantic information provided by SUIM, we design a new multi-stage multi-task learning framework to predict dense relative depth map for single underwater image. Comprehensive comparison and ablation study on the publicly available dataset and our new benchmark demonstrate the effectiveness of the proposed weakly-supervised strategy for dense relative depth estimation. The new benchmark, source code and pre-train models are both available at the project home page: https://github.com/.</em></p>
                        <p class="text-left">&nbsp;</p>
                        <h5 class="text-center">
                            <a href="https://arxiv.org/abs/2201.02832">[Paper]</a>
                            <a href="https://github.com/trentqq/SGUIE-Net_Simple">[Code]</a>
                            <a href="./SGUIE-Net_files/SGUIE-Net_Suppy.pdf">[Supplementary]</a>
                            <a href="https://drive.google.com/drive/folders/1gA3Ic7yOSbHd3w214-AgMI9UleAt4bRM?usp=sharing">[DataSet]</a>
                        </h5>
                    </div>
                </div>

                <hr>
                <div class="container">
                    <p>&nbsp;</p>
                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
                            <h2>Highlights</h2>
                            <ol>
                                <li>
                                    <p class="text-left">To facilitate research on underwater image depth estima- tion, we establish a new benchmark, i.e., SUIM-SDA, by extending the Segmentation of Underwater IMagery (SUIM) dataset with Sparse Depth Annotation (SDA).
                                        SUIM-SDA is the first large-scale underwater image depth estimation benchmark, containing a total of 1596 images, more than 6,000 depth trendlines and 25 million point pairs with relative depth labels.
                                    </p>
                                </li>
                                <li>
                                    <p class="text-left">We design three adaptive generation strategies for depth-ranked samples based on depth trendlines. Diverse sam- ples with depth-rank labels can be generated efficiently at low annotation cost,
                                        thus facilitating a weakly supervised learning strategy for dense depth maps with depth trendlines as supervised information.</p>
                                </li>
                                <li>
                                    <p class="text-left">We design a weakly-supervised and two-stage multi-task knowledge distillation framework for single underwater image depth estimation, namely WsUID-Net, which learns from sparse depth annotation to predict dense relative depth maps.
                                        Extensive experiments and ablation studies on the widely used SQUID and our SUIM- SDA benchmarks have verified the advantages of our approach.</p>
                                </li>
                            </ol>
                        </div>
                    </div>
                </div>
                <div class="container">

                    <hr>
                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-left">
                            <h2>Overall Architecture </h2>
                            <p>&nbsp;</p>
                        </div>

                    </div>
                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12"> <img src="./images/WSUID-Net.png" width="900" alt="">
                            <p>&nbsp;</p>
                            <p class="text-left">Fig 1. The overall architecture of the proposed WsUID-Net. WsUID-Net consists of three branches which are used for depth estimation,
                                semantic segmentation and edge detection respectively, thus implementing a weakly-supervised depth estimation learning framework with the auxiliary supervision of association tasks. In particular,
                                we design a two-stage multi-task feature fusion and distillation strategy, where the task-channel attention model (TCAM) and the proposed complementary weighted fusion module (CWF) are oriented to channel fusion and spatial fusion, respectively.</p>
                            <p>&nbsp;</p>
                            <p>&nbsp;</p>
                        </div>
                        <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12"> <img src="./images/Modules.png" width="900" alt="">
                            <p>&nbsp;</p>
                            <p class="text-left">The detailed structures of the key modules of WsUID-Net. (a) the feature transformation module (FTM) for different tasks; (b) the weighted fusion module (WFM) which is used in the first-stage feature distillation, to fuse the features of two different tasks; (c) the task-channel attention module (TCAM) for the second-stage feature distillation.</p>
                            <p>&nbsp;</p>
                            <p>&nbsp;</p>
                        </div>
                    </div>

                    <hr>
                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-left">
                            <h2>Results </h2>
                            <p>&nbsp;</p>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12"> <img src="./images/SUIM_test.png" width="900" alt="">
                            <p>&nbsp;</p>
                            <p class="text-center">Fig. 3. Visual comparison of depth estimation on images from the test set of the proposed SUIM-SDA dataset. From left to right, (a) the input underwater image, depth estimation results of (b) DCP, (c) UDCP, (d) GDCP, (e) CBF, (f) Hazeline, (g) UW-Net, (h) DIW, (i) PAD-Net, (j) MTAN, (k) MTI-Net  and (l) the proposed WsUID-Net are presented.</p>
                            <p>&nbsp;</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12"> <img src="./images/SQUID_test.png" width="900" alt="">
                            <p>&nbsp;</p>
                            <p class="text-center">Fig 4. Visual comparison of depth estimation on images from the challenging SQUID dataset. From left to right, (a) the input underwater image, depth estimation results of (b) DCP, (c) UDCP, (d) GDCP, (e) CBF, (f) Hazeline, (g) UW-Net, (h) DIW, (i) PAD-Net, (j) MTAN, (k) MTI-Net and (l) the proposed WsUID-Net, and (m) the ground truth maps are presented.
                            </p>
                            <p>&nbsp;</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12"> <img src="./images/middle_result.png" width="900" alt="">
                            <p>&nbsp;</p>
                            <p class="text-center">Fig 5. Visualized intermediate results of each branch. (a) are the input underwater images; (b) and (c) are the initial and middle segmentation masks; (d) and (e) are the initial and middle edge maps; (f)-(h) are the initial, middle and final depth maps.</p>
                            <p>&nbsp;</p>
                        </div>
                    </div>

                    <hr>


                    <!-- <div class="row">
                        <div class="col-lg-12 mb-4 mt-2 text-left">
                            <h2>Demo Video</h2>
                        </div>
                    </div>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
                        <video controls="controls" width="900" height="576" jm_neat="1344787457">
                            <source src="./SGUIE-Net_files/images/supplementary_video.mp4" type="video/mp4">
                        </video>
                        <p>&nbsp;</p>
                    </div>
                    <hr> -->



                    <div class="row"> </div>
                </div>
                <div class="jumbotron">
                    <div class="row">
                        <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-left">
                            <h2>Citation</h2>
                            <br>

                            <pre>@article{qi2022sguienet,
    title={Learning Scribbles for Dense Depth:Weakly-Supervised Single Underwater Image Depth Estimation Boosted by Multi-Task Learning},
    author={Kunqian Li and Xiya Wang and Wenjie Liu and Guojia Hou and Zhiguo Zhang and Kun Sun},
    journal={arXiv preprint arXiv:xxxx.xxxx},
    year={2023}
    }
                            </pre>


                        </div>
                    </div>
                    <!-- <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-left">

                        <p><span style="color:#000000;font-family:&#39;Courier New&#39;;font-size:15px;">

                            </span></p>
                        <p>&nbsp;</p>
                        <p>&nbsp;</p>
                    </div> -->
                </div>

            </div>
        </section>
    </div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="./jquery-3.2.1.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./popper.min.js"></script>
    <script src="./bootstrap-4.0.0.js"></script>


</body>

</html>